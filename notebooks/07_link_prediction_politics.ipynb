{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction with Twitter Friends data  \n",
    "\n",
    "Link prediction is a key problem for network-structured data. Examples of link prediction include predicting friendship links among users in a social network, predicting co-authorship links in a citation network, and predicting interactions between genes and proteins in a biological network.  \n",
    "\n",
    "In this notebook, mostly inspired from chapter 6 of [Graph Machine Learning from Packt Publishing](https://github.com/PacktPublishing/Graph-Machine-Learning), we apply basic Machine Learning tools to data from users followed by the main political leaders in Italy.  \n",
    "\n",
    "We split the graph into a training and testing graph, and then we solve the link prediction task employing and comparing three approaches:\n",
    "* node2vec-based method;\n",
    "* GraphSage algorythm;\n",
    "* Hand-Crafted features method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn import metrics \n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, link_classification\n",
    "from tensorflow import keras\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec.edges import HadamardEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G, pos_nodes, relevant_nodes=[], node_size=3, relevant_node_size=100):\n",
    "    \"\"\"\n",
    "        Draws a Graph and highlights relevant nodes in it.\n",
    "\n",
    "    :param G: graph data;\n",
    "    :param pos_nodes: nodes position in graph (layout);\n",
    "    :param list relevant_nodes: relevant nodes to highlight in the network, default to an empty list;\n",
    "    :param float node_size: size of every node in the network, default to 50 but can also be a list;\n",
    "    :param float relevant_node_size: size of nodes to highlight in the network, defaults to 100 but can also be a list.\n",
    "    \n",
    "    \"\"\"\n",
    "    node_labels = {}\n",
    "    for node in G.nodes():\n",
    "        if node in relevant_nodes:\n",
    "            #set the node name as the key and the label as its value \n",
    "            node_labels[node] = node\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    # set the argument 'with labels' to False so you have unlabeled graph\n",
    "    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='grey')\n",
    "    # draw relevant nodes\n",
    "    nx.draw_networkx_nodes(G, pos_nodes, nodelist=relevant_nodes, node_size=relevant_node_size, node_color='r', alpha=0.5)\n",
    "    # only add labels to the nodes you require\n",
    "    nx.draw_networkx_labels(G, pos_nodes, node_labels, font_size=16, font_color='r', font_weight='bold')\n",
    "    pos_attrs = {}\n",
    "    for node, coords in pos_nodes.items():\n",
    "        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    axis = plt.gca()\n",
    "    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n",
    "    axis.set_ylim([1.2*y for y in axis.get_ylim()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28421, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_net = pd.read_csv('C:/Users/Dylan/Desktop/Projects/working_with_twitter/data/politicians_network.csv')\n",
    "political_net = political_net[['source', 'target']]\n",
    "politicians = list(political_net['source'].unique())\n",
    "political_net.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 24192\n",
      "Number of edges: 28302\n",
      "Average degree:   2.3398\n"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(df=political_net ,source='source',target='target', edge_attr=None, create_using=nx.Graph())\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since graph embeddings algoryhtm are computationally espensive, for the sake of learning we will remove from the graph those users with less than 3 connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 816\n",
      "Number of edges: 3087\n",
      "Average degree:   7.5662\n"
     ]
    }
   ],
   "source": [
    "remove = [node for node,degree in dict(G.degree()).items() if degree < 3]\n",
    "G.remove_nodes_from(nodes=remove)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_graph(G, pos_nodes=nx.kamada_kawai_layout(G), relevant_nodes=politicians, node_size=15, relevant_node_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting with Stellargraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***stellargraph*** library provides a tool for splitting data and creating training and test reduced **subgraphs**.  \n",
    "\n",
    "With the EdgeSplitter class we extract a fraction (p = 10%) of all the edges in G, as well as the same number of negative edges, in order to obtain a reduced graph, graph_test. The train_test_split method also return a lsit of node pairs, samples_test (where each pair corresponds to an existing or not existing edge in the graph) and a list of binary targets (labels_test) of the same length of the samples_test list. Then, from such a reduced graph, we are repeating the operation to obtain another reduced graph (graph_train), as well as the corresponding samples_train, and labels_train lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 308 positive and 308 negative edges. **\n",
      "** Sampled 277 positive and 277 negative edges. **\n"
     ]
    }
   ],
   "source": [
    "edgeSplitter = EdgeSplitter(G)\n",
    "graph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method='global', seed=42)\n",
    "edgeSplitter = EdgeSplitter(graph_test, G)\n",
    "graph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method='global', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 816\n",
      "Number of edges: 2502\n",
      "Average degree:   6.1324\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(graph_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be comparing three different methods for predicting missing edges:  \n",
    "\n",
    "* Method 1: **node2vec** will be used to learn a node embedding without supervision. The learned embedding will be used as input for a supervised classification \n",
    "algorithm to determine whether the input pair is actually connected;\n",
    "* Method 2: The graph neural network-based algorithm **GraphSAGE** will be used to jointly learn the embedding and perform the classification task;  \n",
    "* Method 3: **Hand-crafted features** will be extracted from the graph and used as inputs for a supervised classifier, together with the nodes' IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **node2vec**-based link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use node2vec to generate node embeddings **without supervision** from the \n",
    "training graph. This can be done using the node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 816/816 [00:04<00:00, 178.48it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec = Node2Vec(graph_train, dimensions=64, walk_length=30, num_walks=200, workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use **HadamardEmbedder** for generating an embedding for each pair of \n",
    "embedded nodes. Such feature vectors will be used as input to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform training of our supervised classifier. We will be using the Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(train_embeddings, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the trained model for creating the embedding of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv) \n",
    "test_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test set using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9139784946236559\n",
      "Recall: 0.827922077922078\n",
      "F1-Score: 0.8688245315161841\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(test_embeddings) \n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPHSAGE link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use GraphSAGE for learning node embeddings and classifying edges. We will build a two-layer **GraphSAGE architecture** that, given labeled pairs of nodes, outputs a pair of node embeddings. Then, a fully connected neural network will be used to process these embeddings and produce link predictions. Notice that the GraphSAGE model and the fully connected network will be concatenated and trained end to end so that the embeddings learning stage is influenced by the predictions.  \n",
    "### Featurless Approach\n",
    "GraphSAGE needs node descriptors (features). Such features may or may not be available in your dataset. Let's begin our analysis by not considering available node features. In this case, a common approach is to assign to each node a one-hot feature vector of length |V| (the number of nodes in the graph), where only the cell corresponding to the given node is 1, while the remaining cells are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = np.eye(graph_train.number_of_nodes(), dtype=int)\n",
    "i=0\n",
    "fake_features = {}\n",
    "for n in G.nodes():\n",
    "    fake_features[n] = eye[i]\n",
    "    i = i+1\n",
    "nx.set_node_attributes(graph_train, fake_features, \"fake\")\n",
    "\n",
    "eye = np.eye(graph_test.number_of_nodes(), dtype=int)\n",
    "i=0\n",
    "fake_features = {}\n",
    "for n in G.nodes():\n",
    "    fake_features[n] = eye[i]\n",
    "    i = i+1\n",
    "nx.set_node_attributes(graph_test, fake_features, \"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is repeated for both the training and the testing graph:\n",
    "1. We created an identity matrix of size |V|. Each row of the matrix is the one-hot vector we need for each node in the graph.  \n",
    "2. Then, we created a dictionary where, for each nodeID (used as the key), we assign the corresponding row of the previously created identity matrix.  \n",
    "3. Finally, the dictionary was passed to the networkx set_node_attributes function to assign the \"fake\" features to each node in the networkx graph.  \n",
    "\n",
    "The next step will be defining the generator that will be used to feed the model. We will be using the stellargraph **GraphSAGELinkGenerator** for this, which provides the model with pairs of nodes as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # batch size is the number of inputs for minibatch\n",
    "num_samples = [4, 4] # number of first and second-hop neighbor samples that GraphSage should consider \n",
    "\n",
    "# convert graph_train and graph_test for stellargraph\n",
    "sg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"fake\")\n",
    "sg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"fake\")\n",
    "train_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\n",
    "train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\n",
    "test_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\n",
    "test_flow = test_gen.flow(samples_test, labels_test, seed=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation\n",
    "\n",
    "Create a GraphSAGE model with two hidden layers of size 20, each with a bias term and a dropout layer for reducing overfitting.  \n",
    "\n",
    "Then, the output of the GraphSAGE part of the module is concatenated with a **link_classification layer** that takes pairs of node embeddings (output of GraphSAGE), uses binary operators (*inner product*; ip in our case) to produce edge embeddings, and finally passes them through a fully connected neural network for classification.  \n",
    "\n",
    "The model is optimized via the Adam optimizer (learning rate = 1e-3) using the mean squared error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [20, 20]\n",
    "graphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\n",
    "x_inp, x_out = graphsage.in_out_tensors()\n",
    "# define the link classifier\n",
    "prediction = link_classification(output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\")(x_out)\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "model.compile( optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.mse, metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 294ms/step - loss: 0.2563 - acc: 0.5235 - val_loss: 0.2500 - val_acc: 0.5422\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 0.2460 - acc: 0.5614 - val_loss: 0.2457 - val_acc: 0.5519\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 0.2461 - acc: 0.5740 - val_loss: 0.2365 - val_acc: 0.5877\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.2364 - acc: 0.6011 - val_loss: 0.2355 - val_acc: 0.5893\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.2299 - acc: 0.6480 - val_loss: 0.2358 - val_acc: 0.6104\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.2296 - acc: 0.6155 - val_loss: 0.2336 - val_acc: 0.6169\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2303 - acc: 0.6336 - val_loss: 0.2311 - val_acc: 0.6169\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.2309 - acc: 0.6318 - val_loss: 0.2333 - val_acc: 0.5990\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2280 - acc: 0.6534 - val_loss: 0.2312 - val_acc: 0.6201\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2261 - acc: 0.6661 - val_loss: 0.2304 - val_acc: 0.6315\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(train_flow, epochs=epochs, validation_data=test_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Metrics are lower than in the node2vec approach.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4839506172839506\n",
      "Recall: 0.7075812274368231\n",
      "F1-Score: 0.5747800586510264\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(train_flow)).flatten()\n",
    "print('Precision:', metrics.precision_score(labels_train, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_train, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_train, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSage with Node Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of extracting node features for the combined ego network is quite verbose.  \n",
    "This is because each ego network is described using several files, as well as all the feature names and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_file_name = \"feature_map.txt\"\n",
    "feature_index = {}  # numeric index to name\n",
    "inverted_feature_index = {} # name to numeric index\n",
    "network = nx.Graph()\n",
    "\n",
    "def parse_featname_line(line):\n",
    "  \"\"\" used to parse each line of the files containing feature names \"\"\"\n",
    "  line = line[(line.find(' '))+1:]  # chop first field\n",
    "  split = line.split(';')\n",
    "  name = ';'.join(split[:-1]) # feature name\n",
    "  index = int(split[-1].split(\" \")[-1]) #feature index\n",
    "  return index, name\n",
    "\n",
    "\n",
    "def load_features():\n",
    "  \"\"\" \n",
    "  parse each ego-network and creates two dictionaries:\n",
    "      - feature_index: maps numeric indices to names\n",
    "      - inverted_feature_index: maps names to numeric indices\n",
    "  \"\"\"\n",
    "  import glob\n",
    "  feat_file_name = 'tmp.txt'\n",
    "  # may need to build the index first\n",
    "  if not os.path.exists(feat_file_name):\n",
    "      feat_index = {}\n",
    "      # build the index from data/*.featnames files\n",
    "      featname_files = glob.iglob(\"facebook/*.featnames\")\n",
    "      for featname_file_name in featname_files:\n",
    "          featname_file = open(featname_file_name, 'r')\n",
    "          for line in featname_file:\n",
    "              # example line:\n",
    "              # 0 birthday;anonymized feature 376\n",
    "              index, name = parse_featname_line(line)\n",
    "              feat_index[index] = name\n",
    "          featname_file.close()\n",
    "      keys = feat_index.keys()\n",
    "      keys = sorted(keys)\n",
    "      out = open(feat_file_name,'w')\n",
    "      for key in keys:\n",
    "          out.write(\"%d %s\\n\" % (key, feat_index[key]))\n",
    "      out.close()\n",
    "\n",
    "  index_file = open(feat_file_name,'r')\n",
    "  for line in index_file:\n",
    "      split = line.strip().split(' ')\n",
    "      key = int(split[0])\n",
    "      val = split[1]\n",
    "      feature_index[key] = val\n",
    "  index_file.close()\n",
    "\n",
    "  for key in feature_index.keys():\n",
    "      val = feature_index[key]\n",
    "      inverted_feature_index[val] = key\n",
    "\n",
    "\n",
    "def parse_nodes(network, ego_nodes):\n",
    "  \"\"\"\n",
    "  for each nodes in the network assign the corresponding features \n",
    "  previously loaded using the load_features function\n",
    "  \"\"\"\n",
    "  # parse each node\n",
    "  for node_id in ego_nodes:\n",
    "      featname_file = open(f'facebook/{node_id}.featnames','r')\n",
    "      feat_file     = open(f'facebook/{node_id}.feat','r')\n",
    "      egofeat_file  = open(f'facebook/{node_id}.egofeat','r')\n",
    "      edge_file     = open(f'facebook/{node_id}.edges','r')\n",
    "\n",
    "      ego_features = [int(x) for x in egofeat_file.readline().split(' ')]\n",
    "\n",
    "      # Add ego node features\n",
    "      network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n",
    "      \n",
    "      # parse ego node\n",
    "      i = 0\n",
    "      for line in featname_file:\n",
    "          key, val = parse_featname_line(line)\n",
    "          # Update feature value if necessary\n",
    "          if ego_features[i] + 1 > network.nodes[node_id]['features'][key]:\n",
    "              network.nodes[node_id]['features'][key] = ego_features[i] + 1\n",
    "          i += 1\n",
    "\n",
    "      # parse neighboring nodes\n",
    "      for line in feat_file:\n",
    "          featname_file.seek(0)\n",
    "          split = [int(x) for x in line.split(' ')]\n",
    "          node_id = split[0]\n",
    "          features = split[1:]\n",
    "\n",
    "          # Add node features\n",
    "          network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n",
    "\n",
    "          i = 0\n",
    "          for line in featname_file:\n",
    "              key, val = parse_featname_line(line)\n",
    "              # Update feature value if necessary\n",
    "              if features[i] + 1 > network.nodes[node_id]['features'][key]:\n",
    "                  network.nodes[node_id]['features'][key] = features[i] + 1\n",
    "              i += 1\n",
    "          \n",
    "      featname_file.close()\n",
    "      feat_file.close()\n",
    "      egofeat_file.close()\n",
    "      edge_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-Crafted Features for Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each input edge, we will compute a set of metrics that will be given as input to a classifier.  \n",
    "\n",
    "In this example, for each input edge represented as a pair of nodes (u,v), four metrics will be considered, namely the following\n",
    "\n",
    "* **Shortest path**: The length of the shortest path between u and v. If u and v are directly connected through an edge, this edge will be removed before computing the shortest path. The value 0 will be used if u is not reachable from v;\n",
    "* **Jaccard coefficient**: Given a pair of nodes (u,v), it is defined as the intersection over a union of the set of neighbors of u and v;\n",
    "* **u centrality**: The degree centrality computed for node v;\n",
    "* **v centrality**: The degree centrality computed for node u;\n",
    "* **u community**: The community ID assigned to node u using the Louvain heuristic;\n",
    "* **v community**: The community ID assigned to node v using the Louvain heuristic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(G,u,v):\n",
    "  \"\"\" \n",
    "  return the shortest path length between u,v in the graph\n",
    "  without the edge (u,v) \n",
    "  \"\"\"\n",
    "  removed = False\n",
    "  if G.has_edge(u,v):\n",
    "    removed = True\n",
    "    G.remove_edge(u,v) # temporary remove edge\n",
    "  try:\n",
    "    sp = len(nx.shortest_path(G, u, v))\n",
    "  except:\n",
    "    sp = 0\n",
    "  if removed:\n",
    "    G.add_edge(u,v) # add back the edge if it was removed\n",
    "  return sp\n",
    "\n",
    "def get_hc_features(G, samples_edges, labels):\n",
    "  # precompute metrics\n",
    "  centralities = nx.degree_centrality(G)\n",
    "  parts = community.best_partition(G)\n",
    "  feats = []\n",
    "  for (u,v),l in zip(samples_edges, labels):\n",
    "    shortest_path = get_shortest_path(G, u, v)\n",
    "    j_coefficient = next(nx.jaccard_coefficient(G, ebunch=[(u, v)]))[-1]\n",
    "    u_centrality = centralities[u]\n",
    "    v_centrality = centralities[v]\n",
    "    u_community = parts.get(u)\n",
    "    v_community = parts.get(v)\n",
    "    # add the feature vector\n",
    "    feats += [[shortest_path, j_coefficient, u_centrality, v_centrality]]\n",
    "  return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = get_hc_features(graph_train, samples_train, labels_train)\n",
    "feat_test = get_hc_features(graph_test, samples_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train again a RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10) \n",
    "rf.fit(feat_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9967532467532467\n",
      "Recall: 0.9967532467532467\n",
      "F1-Score: 0.9967532467532467\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(feat_test)\n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred))\n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOW**!!!\n",
    "\n",
    "Summing up the results, the node2vec-based method is already able to achieve a high level of performance without supervision and per-node information. Such high results might be related to the particular structure of the combined ego network. Due to the high sub-modularity of the network (since it is composed of several ego networks), predicting whether two users will be connected or not might be highly related to the way the two candidate nodes are connected inside the network. For example, there might be a systematic situation in which two users, both connected to several users in the same \n",
    "ego network, have a high chance of being connected as well. On the other hand, two users belonging to different ego networks, or very far from each other, are likely to not be connected, making the prediction task easier. This is also confirmed by the high results \n",
    "achieved using the shallow method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('GraphVenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "227f9a8b37604bdc5d7dc3398ed71e8d10bcc6d6be940effa20b0215c0cfeb4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
